{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning üîß\n",
    "\n",
    "https://docs.ray.io/en/latest/tune/index.html\n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "One of the heaviest computing task in Machine Learning is hyperparameter tuning. Fortunately for us, this is also inheritely parallelizable which makes it a perfect candidate for a Ray job. Therefore, we will cover:\n",
    "\n",
    "- How to perform Ray Tune for `sklearn`\n",
    "- Analyse hyperparameter tuning jobs results\n",
    "\n",
    "## Ray Tune Locally\n",
    "\n",
    "Now just like we did previously, before playing with our now up and running ray cluster (if it is not refer to the previous lecture), let's discover ray tune on a local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune is a library for hyperparameter tuning at any scale. With Tune, you can launch a multi-node distributed hyperparameter sweep in less than 10 lines of code. Tune supports any deep learning framework, including PyTorch, TensorFlow, and Keras.\n",
    "Open a terminal, and activate you ray virtual environment (if you do not have it, refer to the previous lecture)\n",
    "\n",
    "```shell\n",
    "conda activate ray\n",
    "```\n",
    "\n",
    "`pip install -U \"ray[tune]\" optuna`\n",
    "\n",
    "This example runs a small grid search with an iterative training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"ray[tune]\" optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "\n",
    "\n",
    "def objective(config):  # ‚ë†\n",
    "    score = config[\"a\"] ** 2 + config[\"b\"]\n",
    "    return {\"score\": score}\n",
    "\n",
    "\n",
    "search_space = {  # ‚ë°\n",
    "    \"a\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\n",
    "    \"b\": tune.choice([1, 2, 3]),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(objective, param_space=search_space)  # ‚ë¢\n",
    "\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"score\", mode=\"min\").config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this really was just the toy example, let's now attempt somthing just a little more ML related by searching the optimal number of neurons on the first layer of a neural network using pytorch and optuna (an hyperparameter tuning library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "from ray import train, tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "\n",
    "def objective(config):  # ‚ë†\n",
    "    # Data\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    test_data = MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "    # MODEL\n",
    "    # Define the CNN architecture\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, units):\n",
    "            super(CNN, self).__init__()\n",
    "            # First convolutional layer: 1 input channel (grayscale), 32 output channels, 3x3 kernel\n",
    "            self.conv1 = nn.Conv2d(1, units, kernel_size=3, stride=1, padding=1)\n",
    "            # Second convolutional layer: 32 input channels, 64 output channels, 3x3 kernel\n",
    "            self.conv2 = nn.Conv2d(units, units*2, kernel_size=3, stride=1, padding=1)\n",
    "            # Fully connected layer 1 (after flattening): input 7*7*64, output 128\n",
    "            self.fc1 = nn.Linear(units*2 * 7 * 7, 128)\n",
    "            # Fully connected layer 2: input 128, output 10 (for 10 classes)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "            # Max pooling layer\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            # Dropout layer to prevent overfitting\n",
    "            self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Apply first conv layer + ReLU + max pool\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            # Apply second conv layer + ReLU + max pool\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            # Flatten the output for the fully connected layers\n",
    "            x = x.view(-1, self.conv2.out_channels * 7 * 7)\n",
    "            # Apply dropout, first fully connected layer + ReLU\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            # Output layer (no activation function for the output, since we'll apply CrossEntropyLoss)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = CNN(units=config[\"units\"]).to(\"cpu\")  # Create a PyTorch conv net\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training the model\n",
    "    def train_model(num_epochs):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                # Optimize\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Testing the model\n",
    "    def test_model():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return correct / total\n",
    "    \n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    while True:\n",
    "        train_model(1)  # Train the model\n",
    "        acc = test_model()  # Compute test accuracy\n",
    "        train.report({\"mean_accuracy\": acc})  # Report to Tune\n",
    "\n",
    "\n",
    "search_space = {\"units\": tune.qrandint(8,64)}\n",
    "algo = OptunaSearch()  # ‚ë°\n",
    "\n",
    "tuner = tune.Tuner(  # ‚ë¢\n",
    "    objective,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        search_alg=algo,\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"training_iteration\": 100},\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()\n",
    "print(\"Best config is:\", results.get_best_result().config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Resources üìöüìö\n",
    "\n",
    "[Ray tune](https://docs.ray.io/en/latest/tune/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
