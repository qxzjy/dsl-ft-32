{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray tune on the ray cluster\n",
    "\n",
    "Now let's run a hyperparameter tuning job using the ray cluster.\n",
    "Hyperparameter tuning jobs may quickly request important amounts of resources to our cluster, therefore we will need to reinstall the cluster with higher resource levels.\n",
    "\n",
    "```shell\n",
    "helm uninstall raycluster\n",
    "```\n",
    "\n",
    "Then let's use the following yaml file (we called it `ray-cluster.yaml`) in order to upgrade our new installation:\n",
    "\n",
    "```yaml\n",
    "head:\n",
    "  enableInTreeAutoscaling: true\n",
    "  resources:\n",
    "    limits:\n",
    "      cpu: \"2\"\n",
    "      # To avoid out-of-memory issues, never allocate less than 2G memory for the Ray head.\n",
    "      memory: \"5G\"\n",
    "    requests:\n",
    "      cpu: \"2\"\n",
    "      memory: \"5G\"\n",
    "\n",
    "\n",
    "worker:\n",
    "  replicas: 1\n",
    "  resources:\n",
    "    limits:\n",
    "      cpu: \"2\"\n",
    "      memory: \"5G\"\n",
    "    requests:\n",
    "      cpu: \"2\"\n",
    "      memory: \"5G\"\n",
    "```\n",
    "\n",
    "With this done let's re-install the ray cluster.\n",
    "\n",
    "```shell\n",
    "helm install raycluster kuberay/ray-cluster --version 1.0.0 -f ray-cluster.yaml \n",
    "```\n",
    "\n",
    "Now our cluster should be packed with enough power to deal with the job described in the following script:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "from ray import train, tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "\n",
    "def objective(config):  # â‘ \n",
    "    # Data\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    test_data = MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "    # MODEL\n",
    "    # Define the CNN architecture\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, units):\n",
    "            super(CNN, self).__init__()\n",
    "            # First convolutional layer: 1 input channel (grayscale), 32 output channels, 3x3 kernel\n",
    "            self.conv1 = nn.Conv2d(1, units, kernel_size=3, stride=1, padding=1)\n",
    "            # Second convolutional layer: 32 input channels, 64 output channels, 3x3 kernel\n",
    "            self.conv2 = nn.Conv2d(units, units*2, kernel_size=3, stride=1, padding=1)\n",
    "            # Fully connected layer 1 (after flattening): input 7*7*64, output 128\n",
    "            self.fc1 = nn.Linear(units*2 * 7 * 7, 128)\n",
    "            # Fully connected layer 2: input 128, output 10 (for 10 classes)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "            # Max pooling layer\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            # Dropout layer to prevent overfitting\n",
    "            self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Apply first conv layer + ReLU + max pool\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            # Apply second conv layer + ReLU + max pool\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            # Flatten the output for the fully connected layers\n",
    "            x = x.view(-1, self.conv2.out_channels * 7 * 7)\n",
    "            # Apply dropout, first fully connected layer + ReLU\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            # Output layer (no activation function for the output, since we'll apply CrossEntropyLoss)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = CNN(units=config[\"units\"]).to(\"cpu\")  # Create a PyTorch conv net\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training the model\n",
    "    def train_model(num_epochs):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                # Optimize\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Testing the model\n",
    "    def test_model():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return correct / total\n",
    "    \n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    while True:\n",
    "        train_model(1)  # Train the model\n",
    "        acc = test_model()  # Compute test accuracy\n",
    "        train.report({\"mean_accuracy\": acc})  # Report to Tune\n",
    "\n",
    "\n",
    "search_space = {\"units\": tune.qrandint(8,64)}\n",
    "algo = OptunaSearch()  # â‘¡\n",
    "\n",
    "tuner = tune.Tuner(  # â‘¢\n",
    "    objective,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        search_alg=algo,\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"training_iteration\": 100},\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()\n",
    "print(\"Best config is:\", results.get_best_result().config)\n",
    "```\n",
    "\n",
    "Now submit your job by running:\n",
    "\n",
    "```bash\n",
    "ray job submit --runtime-env-json='{\"working_dir\": \"./\", \"pip\": [\"ray[tune]\", \"numpy\", \"joblib\", \"torch\", \"torchvision\", \"optuna\"]}' --address=\"http://127.0.0.1:8265\" -- python ray_tune_demo.py\n",
    "```\n",
    "\n",
    "## Resources ðŸ“šðŸ“š\n",
    "\n",
    "- [Ray tune](https://docs.ray.io/en/latest/tune/index.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
