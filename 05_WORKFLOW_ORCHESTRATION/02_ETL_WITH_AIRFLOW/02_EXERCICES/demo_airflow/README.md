# ðŸš€ DÃ©ploiement d'Airflow pour votre premiÃ¨re pipeline

## ðŸŽ¯ Objectif

Ce guide explique comment dÃ©ployer **Airflow** avec **Docker Compose**, et configurer les connexions Ã  **PostgreSQL** et **AWS S3** directement dans lâ€™interface Airflow pour dÃ©ployer votre premiÃ¨re vraie pipeline !

---

## ðŸ›  PrÃ©requis

- **Docker & Docker Compose** installÃ©s
- Un **bucket S3** (Ã  crÃ©er)
- Une **base de donnÃ©es PostgreSQL** (Ã  crÃ©er via NeonDB, Kubernetes, ou Docker)

---

## ðŸ“Œ 1. Dockerfile (Image Airflow)

Le fichier `Dockerfile` utilisÃ© pour construire lâ€™image Airflow :

```dockerfile
FROM apache/airflow:2.10.4-python3.10

USER root  # Passer en root pour installer les dÃ©pendances

RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq-dev \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

USER airflow  # Revenir Ã  lâ€™utilisateur airflow pour la sÃ©curitÃ©

# Copier les dÃ©pendances Python
COPY requirements.txt .

# Installer les dÃ©pendances Python
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir --upgrade --no-build-isolation -r requirements.txt
```

---

## ðŸ“Œ 2. requirements.txt (DÃ©pendances Python)

Les packages requis :

```
apache-airflow-providers-postgres
apache-airflow-providers-amazon
psycopg[binary]
pandas
```

---

## ðŸ“Œ 3. docker-compose.yaml (DÃ©ploiement Airflow)

RÃ©cuperez le docker-compose du zip.

---

## ðŸ“Œ 4. DÃ©marrage du Serveur Airflow

1. Lancer les conteneurs : (pas besoin de build c'est fait dans le docker compose)

```bash
docker-compose up airflow-init
```

```bash
docker-compose up
```

2. AccÃ©der Ã  Airflow :
   - Ouvrez http://localhost:8080.
   - Connectez-vous avec airflow / airflow.

---

## ðŸ“Œ 5. Configuration des Connexions dans Airflow

### Connexion AWS (S3)

1. Admin > Connections dans Airflow.
2. CrÃ©ez une connexion avec :

   - Conn Id : aws_default
   - Conn Type : Amazon Web Services
   - AWS Access Key ID : VOTRE_ACCESS_KEY
   - AWS Secret Access Key : VOTRE_SECRET_KEY
   - Extra :

   ```json
   {
     "region_name": "VOTRE_REGION"
   }
   ```

3. Sauvegardez.

### Connexion PostgreSQL

1. Admin > Connections dans Airflow.
2. CrÃ©ez une connexion avec :
   - Conn Id : postgres_default
   - Conn Type : Postgres
   - Host : VOTRE_HOST
   - Database : VOTRE_BDD
   - Login : VOTRE_UTILISATEUR
   - Password : VOTRE_MOT_DE_PASSE
   - Port : 5432
   - Extra :
   ```json
   {
     "sslmode": "require"
   }
   ```
3. Sauvegardez.

---

## ðŸ“Œ 6. Configuration des Variables d'Environnement dans Airflow

Dans Airflow, les variables dâ€™environnement peuvent Ãªtre dÃ©finies directement via lâ€™interface web.

### Ã‰tapes :

1. AccÃ©dez Ã  l'interface Airflow (http://localhost:8080).
2. Allez dans :
   Admin â†’ Variables â†’ "+" (Ajouter une variable)"
3. Ajoutez les variables suivantes :
   - S3BucketName â†’ Nom du bucket S3
   - WeatherBitApiKey â†’ ClÃ© API WeatherBit
4. Exemple de configuration :
   | ClÃ© | Valeur |
   | :--------------- | :-----------------: |
   | S3BucketName | nom-de-votre-bucket |
   | WeatherBitApiKey | votre-clÃ©-api |

5. Cliquez sur "Enregistrer".

---

## ðŸ“Œ 7. Vous pouvez maintenant trigger votre dags !
