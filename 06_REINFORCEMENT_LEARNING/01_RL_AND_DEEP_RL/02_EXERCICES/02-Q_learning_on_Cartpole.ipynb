{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0cbf2b",
   "metadata": {},
   "source": [
    "# Q learning on Cart Pole\n",
    "\n",
    "This exercise will focus on solving the reinforcement learning problem for the Cart Pole environment.\n",
    "\n",
    "## Description\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "## Action Space\n",
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "Action:\n",
    "* 0: Push cart to the left\n",
    "* 1: Push cart to the right\n",
    "\n",
    "Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "## Observation Space\n",
    "The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\n",
    "\n",
    "Num\n",
    "\n",
    "Observation\n",
    "\n",
    "Min\n",
    "\n",
    "Max\n",
    "\n",
    "* 0: Cart Position $\\in [-4.8, 4.8]$\n",
    "* 1: Cart Velocity $\\in [-\\infty, \\infty]$\n",
    "* 2: Pole Angle $\\in [~ -0.418 rad (-24°), ~ 0.418 rad (24°)]$\n",
    "* 3: Pole Angular Velocity $\\in [-\\infty,\\infty]$\n",
    "\n",
    "Note: While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "\n",
    "The cart x-position (index 0) can take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\n",
    "\n",
    "The pole angle can be observed between (-.418, .418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-.2095, .2095) (or ±12°)\n",
    "\n",
    "## Rewards\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "## Starting State\n",
    "All observations are assigned a uniformly random value in (-0.05, 0.05)\n",
    "\n",
    "## Episode Termination\n",
    "The episode terminates if any one of the following occurs:\n",
    "\n",
    "* Pole Angle is greater than ±12°\n",
    "* Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "* Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44503c",
   "metadata": {},
   "source": [
    "1. We'll start by installing the gym libraries needed for simulating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee9a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/charles/.local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/charles/.local/lib/python3.10/site-packages (from gym) (1.26.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/charles/.local/lib/python3.10/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/charles/.local/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: gym[classic_control] in /home/charles/.local/lib/python3.10/site-packages (0.26.2)\n",
      "\u001b[33mWARNING: gym 0.26.2 does not provide the extra 'classic-control'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /home/charles/.local/lib/python3.10/site-packages (from gym[classic_control]) (1.26.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/charles/.local/lib/python3.10/site-packages (from gym[classic_control]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/charles/.local/lib/python3.10/site-packages (from gym[classic_control]) (0.0.8)\n",
      "Requirement already satisfied: pygame==2.1.0 in /home/charles/anaconda3/envs/rl/lib/python3.10/site-packages (from gym[classic_control]) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaf029",
   "metadata": {},
   "source": [
    "2. We'll also proceed to the needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69c3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "\n",
    "import gym # pull the environment\n",
    "\n",
    "import time # to measure execution time\n",
    "\n",
    "import math # needed for calculations\n",
    "\n",
    "import matplotlib.pyplot as plt # for visualizing\n",
    "\n",
    "import pygame # has some effect on the rendering\n",
    "\n",
    "import matplotlib.animation as animation # for making gifs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ec3b3",
   "metadata": {},
   "source": [
    "3. Let's create a variable `env` using the method described in the [documentation](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8b79af-467d-4848-a59e-45ba75459e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c540868",
   "metadata": {},
   "source": [
    "4. take a look at the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59714371-9500-4d7f-a0e3-3e9cb99b20ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c30209",
   "metadata": {},
   "source": [
    "5. Take a look at the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3230ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7cbc56",
   "metadata": {},
   "source": [
    "6. Reset the environment to take a look at a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c97e0ee-6635-4d7d-bafc-0364720e75cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02276522,  0.03296215, -0.04455954,  0.04662376], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "# cart position , cart velocity, pole angle, pole angular velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba6392",
   "metadata": {},
   "source": [
    "7. Since the environment is continuous in time, we'll make gifs to be able to visualize what the agent is doing. We'll take a few steps to do this:\n",
    "    * Reset the environment\n",
    "    * Define an empty list called `arr`\n",
    "    * Set a variable `done` with value `False`\n",
    "    * Set a variable `i` with value `0`\n",
    "    * Start a while loop that will continue as long as `done` is `False` in the loop do:\n",
    "        * append the visualization of the current state of the environment to the `arr` list (using `env.render(mode='rgb_array')` it should be a numpy array)\n",
    "        * take a random action to produce the new observation\n",
    "        * print the new observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6036201c-34b5-4af9-851d-71ead57f6705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 [-0.01759298  0.23302849  0.04547037 -0.2892673 ] 1.0 False {}\n",
      "step 2 [-0.01293241  0.03728863  0.03968502  0.01740269] 1.0 False {}\n",
      "step 3 [-0.01218664 -0.15837932  0.04003308  0.32233787] 1.0 False {}\n",
      "step 4 [-0.01535423 -0.3540478   0.04647983  0.62737197] 1.0 False {}\n",
      "step 5 [-0.02243518 -0.5497866   0.05902727  0.9343233 ] 1.0 False {}\n",
      "step 6 [-0.03343092 -0.35550848  0.07771374  0.66075754] 1.0 False {}\n",
      "step 7 [-0.04054108 -0.16154903  0.09092889  0.39352134] 1.0 False {}\n",
      "step 8 [-0.04377206  0.03217288  0.09879932  0.13083519] 1.0 False {}\n",
      "step 9 [-0.04312861  0.22575094  0.10141602 -0.12911612] 1.0 False {}\n",
      "step 10 [-0.03861359  0.02933345  0.0988337   0.1937615 ] 1.0 False {}\n",
      "step 11 [-0.03802692 -0.1670532   0.10270893  0.5159137 ] 1.0 False {}\n",
      "step 12 [-0.04136799 -0.3634601   0.1130272   0.83911484] 1.0 False {}\n",
      "step 13 [-0.04863719 -0.17004791  0.1298095   0.584007  ] 1.0 False {}\n",
      "step 14 [-0.05203814  0.02303957  0.14148964  0.33486953] 1.0 False {}\n",
      "step 15 [-0.05157735  0.21589413  0.14818703  0.08993776] 1.0 False {}\n",
      "step 16 [-0.04725947  0.40861562  0.14998579 -0.1525685 ] 1.0 False {}\n",
      "step 17 [-0.03908716  0.21169986  0.14693442  0.1834197 ] 1.0 False {}\n",
      "step 18 [-0.03485316  0.01481445  0.1506028   0.518609  ] 1.0 False {}\n",
      "step 19 [-0.03455687  0.20753099  0.160975    0.2769163 ] 1.0 False {}\n",
      "step 20 [-0.03040625  0.40003413  0.16651331  0.03901725] 1.0 False {}\n",
      "step 21 [-0.02240557  0.20296441  0.16729365  0.37926224] 1.0 False {}\n",
      "step 22 [-0.01834628  0.39536467  0.17487891  0.14364623] 1.0 False {}\n",
      "step 23 [-0.01043899  0.19822569  0.17775182  0.48599717] 1.0 False {}\n",
      "step 24 [-0.00647447  0.00109957  0.18747178  0.8290117 ] 1.0 False {}\n",
      "step 25 [-0.00645248  0.19323209  0.204052    0.6006569 ] 1.0 False {}\n",
      "step 26 [-0.00258784 -0.00407157  0.21606514  0.95004827] 1.0 True {}\n",
      "step 27 [-0.00266927 -0.20134443  0.23506612  1.3021073 ] 0.0 True {}\n",
      "step 28 [-0.00669616 -0.00984569  0.26110825  1.0912335 ] 0.0 True {}\n",
      "step 29 [-0.00689307 -0.20728177  0.28293294  1.4532459 ] 0.0 True {}\n",
      "step 30 [-0.01103871 -0.4045481   0.31199783  1.8194575 ] 0.0 True {}\n",
      "step 31 [-0.01912967 -0.60151047  0.348387    2.190884  ] 0.0 True {}\n",
      "step 32 [-0.03115988 -0.41101608  0.39220467  2.022675  ] 0.0 True {}\n",
      "step 33 [-0.0393802  -0.6075766   0.43265817  2.4075027 ] 0.0 True {}\n",
      "step 34 [-0.05153174 -0.41798165  0.48080823  2.2725856 ] 0.0 True {}\n",
      "step 35 [-0.05989137 -0.6135925   0.52625996  2.6687086 ] 0.0 True {}\n",
      "step 36 [-0.07216322 -0.8078667   0.57963413  3.0683668 ] 0.0 True {}\n",
      "step 37 [-0.08832055 -0.6184439   0.64100146  2.991671  ] 0.0 True {}\n",
      "step 38 [-0.10068943 -0.42990735  0.7008349   2.9408154 ] 0.0 True {}\n",
      "step 39 [-0.10928757 -0.24212654  0.7596512   2.9151206 ] 0.0 True {}\n",
      "step 40 [-0.11413011 -0.43210742  0.8179536   3.3242152 ] 0.0 True {}\n",
      "step 41 [-0.12277225 -0.24361831  0.8844379   3.3454516 ] 0.0 True {}\n",
      "step 42 [-0.12764463 -0.05532474  0.95134693  3.3938885 ] 0.0 True {}\n",
      "step 43 [-0.12875111 -0.23915938  1.0192246   3.7933607 ] 0.0 True {}\n",
      "step 44 [-0.1335343 -0.4191708  1.0950919  4.1852574] 0.0 True {}\n",
      "step 45 [-0.14191772 -0.2258731   1.178797    4.3138294 ] 0.0 True {}\n",
      "step 46 [-0.14643519 -0.03120168  1.2650737   4.473971  ] 0.0 True {}\n",
      "step 47 [-0.14705922 -0.2005486   1.3545531   4.830794  ] 0.0 True {}\n",
      "step 48 [-1.5107019e-01 -1.8107326e-04  1.4511689e+00  5.0534596e+00] 0.0 True {}\n",
      "step 49 [-0.15107381  0.2033012   1.5522381   5.308933  ] 0.0 True {}\n",
      "step 50 [-0.14700778  0.41049442  1.6584167   5.5971146 ] 0.0 True {}\n",
      "step 51 [-0.1387979   0.25813207  1.770359    5.869987  ] 0.0 True {}\n",
      "step 52 [-0.13363525  0.4738275   1.8877589   6.222292  ] 0.0 True {}\n",
      "step 53 [-0.1241587  0.328448   2.0122046  6.4336786] 0.0 True {}\n",
      "step 54 [-0.11758974  0.5522354   2.1408782   6.842906  ] 0.0 True {}\n",
      "step 55 [-0.10654504  0.40949076  2.2777364   6.974853  ] 0.0 True {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rl-env/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# returns an initial observation\n",
    "env.reset()\n",
    "\n",
    "arr = []\n",
    "done_count = 0\n",
    "i = 0\n",
    "while done_count < 30 :\n",
    "    # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "    arr.append(env.render())\n",
    "    observation, reward, done, _, info = env.step(env.action_space.sample())\n",
    "    i+=1\n",
    "    if done :\n",
    "        done_count+=1\n",
    "    print(\"step\", i, observation, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ea873",
   "metadata": {},
   "source": [
    "8. Use the following command to make a gif out of the `arr` list of renderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35702705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "imgs = arr\n",
    "imgs = [Image.fromarray(img) for img in imgs]\n",
    "# duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "imgs[0].save(\"cart_pole_not_trained.gif\", save_all=True, append_images=imgs[1:], duration=50, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86c32b",
   "metadata": {},
   "source": [
    "9. Since the observation space is continuous and not discrete, we cannot straightforwardly apply Q-learning. The trick here is to convert this continuous state reinforcement learning problem into a discrete reinforcement learning problem by splitting the range of the different observation metrics into categories. Let's do that.\n",
    "* What's the legal range of the four different metrics that define a non terminal state?\n",
    "* For making the discretization function though, do no hesitate to assume larger ranges to avoid any errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e6028",
   "metadata": {},
   "source": [
    "* 0: Cart Position $\\in [-4.8, 4.8]$ but valid values are $\\in [-2.4, 2.4]$ round it to $[-3, 3]$\n",
    "* 1: Cart Velocity $\\in [-\\infty, \\infty]$ but based on the observations they should be $\\in [-5,5]$\n",
    "* 2: Pole Angle $\\in [~ -0.418 rad (-24°), ~ 0.418 rad (24°)]$ but valid values are $\\in [-.2095, .2095]$ let's round it to $[-.3, .3]$\n",
    "* 3: Pole Angular Velocity $\\in [-\\infty,\\infty]$ but based on the observations they should be $\\in [-5,5]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774ee01",
   "metadata": {},
   "source": [
    "10. We would like to split all of our state variables into 51 categories, set up an `Observation` object that is a list `[51,51,51,51]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffec934",
   "metadata": {},
   "outputs": [],
   "source": [
    "Observation = [51, 51, 51, 51]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b1c60",
   "metadata": {},
   "source": [
    "11. We need to define a function (we'll call it `get_discrete_state`) that turns a continuous state observation into a discrete state observation. The idea is to have 50 categories with category 0 corresponding to the lowest value, and 50 being the highest value. The input from the function should be a state, and the output should be a tuple of integers between 0 and 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962c63c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(25), np.int64(24), np.int64(25), np.int64(25))\n",
      "(np.int64(0), np.int64(0), np.int64(0), np.int64(0))\n",
      "(np.int64(50), np.int64(50), np.int64(50), np.int64(50))\n"
     ]
    }
   ],
   "source": [
    "# setup a random state, a minimum legal state and maximum legal state\n",
    "state, info = env.reset()\n",
    "state_min = np.array([-3,-5,-0.3,-5])\n",
    "state_max = np.array([3,5,0.3,5])\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    var_range = (state_max - state_min)\n",
    "    discrete_state = (state - state_min) / var_range *50\n",
    "    return tuple(discrete_state.astype(int))\n",
    "\n",
    "print(get_discrete_state(state))\n",
    "print(get_discrete_state(state_min))\n",
    "print(get_discrete_state(state_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defcb49",
   "metadata": {},
   "source": [
    "12. Create the Q table for the discretized reinforcement learning problem, initialize it with zeros. What is its shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f3f15f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 51, 51, 51, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros(shape=(Observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31163f91",
   "metadata": {},
   "source": [
    "13. Let's now initialize the values we will need for the Q-learning algorithm to work:\n",
    "* `LEARNING_RATE` = 0.1\n",
    "* `DISCOUNT` = 0.95\n",
    "* `EPISODES` = 60000\n",
    "* `total` = 0\n",
    "* `total_reward` = 0\n",
    "* `prior_reward` = 0\n",
    "* `epsilon` = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a96ad43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 60000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5c52e",
   "metadata": {},
   "source": [
    "14. Let's now code the Q-learning algorithm, here are the steps:\n",
    "* Loop over the number of episodes\n",
    "    * reset the environment\n",
    "    * discretize the initial state\n",
    "    * setup `done=False`\n",
    "    * setup `episode_reward=0`\n",
    "    * loop until `done` is `True`\n",
    "        * setup conditions to implement the $\\epsilon-greedy$ policy\n",
    "        * take a step with the chosen action\n",
    "        * increment the `episode_reward`\n",
    "        * discretize the new state\n",
    "        * if the state is not terminal:\n",
    "            * update the Q-table the Q-learning update rule\n",
    "        * replace current discrete state with new discrete state\n",
    "\n",
    "As sanity check, print the average `episode_reward` calculated over the last 1000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ea39e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For episode: 0 the reward was: 60.0\n",
      "For episode: 1000 the reward was: 28.098\n",
      "For episode: 2000 the reward was: 30.759\n",
      "For episode: 3000 the reward was: 34.135\n",
      "For episode: 4000 the reward was: 38.699\n",
      "For episode: 5000 the reward was: 42.431\n",
      "For episode: 6000 the reward was: 48.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rl-env/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/rl-env/lib/python3.11/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For episode: 7000 the reward was: 54.87\n",
      "For episode: 8000 the reward was: 58.06\n",
      "For episode: 9000 the reward was: 63.654\n",
      "For episode: 10000 the reward was: 70.68\n",
      "For episode: 11000 the reward was: 83.143\n",
      "For episode: 12000 the reward was: 88.052\n",
      "For episode: 13000 the reward was: 95.935\n",
      "For episode: 14000 the reward was: 92.65\n",
      "For episode: 15000 the reward was: 99.459\n",
      "For episode: 16000 the reward was: 97.041\n",
      "For episode: 17000 the reward was: 100.747\n",
      "For episode: 18000 the reward was: 108.496\n",
      "For episode: 19000 the reward was: 105.037\n",
      "For episode: 20000 the reward was: 108.05\n",
      "For episode: 21000 the reward was: 105.993\n",
      "For episode: 22000 the reward was: 110.378\n",
      "For episode: 23000 the reward was: 113.177\n",
      "For episode: 24000 the reward was: 116.374\n",
      "For episode: 25000 the reward was: 112.859\n",
      "For episode: 26000 the reward was: 116.942\n",
      "For episode: 27000 the reward was: 122.548\n",
      "For episode: 28000 the reward was: 116.273\n",
      "For episode: 29000 the reward was: 120.953\n",
      "For episode: 30000 the reward was: 121.278\n",
      "For episode: 31000 the reward was: 118.735\n",
      "For episode: 32000 the reward was: 126.243\n",
      "For episode: 33000 the reward was: 128.468\n",
      "For episode: 34000 the reward was: 127.996\n",
      "For episode: 35000 the reward was: 120.151\n",
      "For episode: 36000 the reward was: 123.469\n",
      "For episode: 37000 the reward was: 129.741\n",
      "For episode: 38000 the reward was: 138.424\n",
      "For episode: 39000 the reward was: 134.707\n",
      "For episode: 40000 the reward was: 138.843\n",
      "For episode: 41000 the reward was: 141.668\n",
      "For episode: 42000 the reward was: 143.257\n",
      "For episode: 43000 the reward was: 139.48\n",
      "For episode: 44000 the reward was: 140.665\n",
      "For episode: 45000 the reward was: 143.086\n",
      "For episode: 46000 the reward was: 145.141\n",
      "For episode: 47000 the reward was: 137.396\n",
      "For episode: 48000 the reward was: 140.84\n",
      "For episode: 49000 the reward was: 140.372\n",
      "For episode: 50000 the reward was: 142.323\n",
      "For episode: 51000 the reward was: 143.258\n",
      "For episode: 52000 the reward was: 138.322\n",
      "For episode: 53000 the reward was: 141.071\n",
      "For episode: 54000 the reward was: 144.464\n",
      "For episode: 55000 the reward was: 137.427\n",
      "For episode: 56000 the reward was: 139.586\n",
      "For episode: 57000 the reward was: 147.711\n",
      "For episode: 58000 the reward was: 147.303\n",
      "For episode: 59000 the reward was: 144.196\n",
      "For episode: 60000 the reward was: 149.239\n"
     ]
    }
   ],
   "source": [
    "episode_reward_list = [] # to calculate average reward across the episodes\n",
    "\n",
    "# strting the loop\n",
    "for episode in range(EPISODES+1):\n",
    "    state, info = env.reset()\n",
    "    discrete_state = get_discrete_state(state) # discretize the initial state\n",
    "    done = False # initialize done\n",
    "    episode_reward = 0 # initialize episode reward\n",
    "\n",
    "    while not done: # loop until termination of an episode\n",
    "        if np.random.random() < epsilon: # random action with probability epsilon\n",
    "            action = env.action_space.sample()\n",
    "        elif np.max(q_table[discrete_state]) == np.min(q_table[discrete_state]): # if no best action can be found, pick a random action\n",
    "            action = env.action_space.sample()\n",
    "        else: # pick greedy action with probability 1-epsilon\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "            \n",
    "        new_state, reward, done, _, info = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "        episode_reward += reward #add the reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state) # discretize new state\n",
    "\n",
    "        if not done: #update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state]) # value of the next best action\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)] # value of the current action\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q) # q learning update\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q # set new value in q table\n",
    "\n",
    "        discrete_state = new_discrete_state # replace current state with new state\n",
    "    \n",
    "    episode_reward_list.append(episode_reward) # add the episode reward to the list\n",
    "    \n",
    "    if episode % 1000 == 0: # calculate the average reward across the last 1000 episodes and reinitialize the reward list\n",
    "        print(\"For episode:\", episode, \"the reward was:\", np.mean(episode_reward_list))\n",
    "        episode_reward_list = []\n",
    "        if np.mean(episode_reward_list) > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc03365",
   "metadata": {},
   "source": [
    "15. Now that the training is done, it seems as though things turned out really well! Reuse the code to make the animation in order to display the behaviour of the trained agent on the CartPole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3fa8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an initial observation\n",
    "observation, info = env.reset()\n",
    "\n",
    "arr = []\n",
    "done=False\n",
    "i = 0\n",
    "while not done:\n",
    "    discrete_state = get_discrete_state(observation)\n",
    "    arr.append(env.render())\n",
    "    action = np.argmax(q_table[discrete_state]) #take greedy action\n",
    "    observation, reward, done, _, info = env.step(action)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1278cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = arr\n",
    "imgs = [Image.fromarray(img) for img in imgs]\n",
    "# duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "imgs[0].save(\"cart_pole_trained.gif\", save_all=True, append_images=imgs[1:], duration=50, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737bc02",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully solved a reinforcement learning problem with a continuous state space! The discretization technique is very common when faced with a fairly simple continuous state space like this one! Another approach would be to rely on function approximation in order to map the states and actions to different values in order to pick actions using a function (and this is where Deep Neural Networks may play a significant role!).\n",
    "\n",
    "NB: Note that discretizing a problem like this to use classic Q-learning is in itself already a function approximation ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
